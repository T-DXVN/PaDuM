#!/bin/bash

# # è®¾ç½® GPU
# export CUDA_VISIBLE_DEVICES=0

# # åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

# model_name=S_Mamba
# seq_len=96
# e_layers=2
# d_model=256
# d_state=2
# d_ff=256
# enc_in=7
# dec_in=7
# c_out=7
# data=ETTh1
# root_path="./dataset/ETT-small/"
# data_path="ETTh1.csv"
# features=M
# des='Exp'
# itr=1

# # ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡ï¼ˆå¯é€‰ï¼‰
# declare -A lr_map
# lr_map[96]=0.00007
# lr_map[192]=0.00007
# lr_map[336]=0.00005
# lr_map[720]=0.00005

# # é¢„æµ‹é•¿åº¦åˆ—è¡¨
# declare -a pred_len_list=(96 192 336 720)

# # éåŽ†è®­ç»ƒ
# for pred_len in "${pred_len_list[@]}"; do
#     learning_rate=${lr_map[$pred_len]}
#     model_id=ETTh1_${seq_len}_${pred_len}
#     log_file="./logs/${model_name}_${model_id}.log"
    
#     python -u run.py \
#       --is_training 1 \
#       --root_path "$root_path" \
#       --data_path "$data_path" \
#       --model_id "$model_id" \
#       --model "$model_name" \
#       --data "$data" \
#       --features "$features" \
#       --seq_len $seq_len \
#       --pred_len $pred_len \
#       --e_layers $e_layers \
#       --enc_in $enc_in \
#       --dec_in $dec_in \
#       --c_out $c_out \
#       --des "$des" \
#       --d_model $d_model \
#       --d_state $d_state \
#       --d_ff $d_ff \
#       --itr $itr \
#       --learning_rate $learning_rate > "$log_file" 2>&1

#     echo "âœ… Training completed for ETTh1 | pred_len=$pred_len | LR=$learning_rate | Log: $log_file"
# done

#!/bin/bash

# è®¾ç½® GPU
export CUDA_VISIBLE_DEVICES=0

model_name=S_Mamba
seq_len=96
e_layers=2
d_model=256
d_ff=256
d_state=2
enc_in=7
dec_in=7
c_out=7
data=ETTh2
root_path="./dataset/ETT-small/"
data_path="ETTh2.csv"
features=M
des='Exp'
itr=1

# ä¸åŒ pred_len å¯¹åº”çš„å­¦ä¹ çŽ‡ï¼ˆæ ¹æ®ä½ çš„è®¾ç½®ï¼‰
declare -A lr_map
lr_map[96]=0.00004
lr_map[192]=0.00004
lr_map[336]=0.00003
lr_map[720]=0.00007

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
pred_lens=(96 192 336 720)

# å¾ªçŽ¯æ‰§è¡Œ
for pred_len in "${pred_lens[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    model_id=${data}_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    echo "ðŸš€ å¼€å§‹è®­ç»ƒ: ${model_name} | ${data} | pred_len=${pred_len} | LR=${learning_rate} | æ—¥å¿—: $log_file"

    python -u run.py \
        --is_training 1 \
        --root_path "$root_path" \
        --data_path "$data_path" \
        --model_id "$model_id" \
        --model "$model_name" \
        --data "$data" \
        --features "$features" \
        --seq_len $seq_len \
        --pred_len $pred_len \
        --e_layers $e_layers \
        --enc_in $enc_in \
        --dec_in $dec_in \
        --c_out $c_out \
        --des "$des" \
        --d_model $d_model \
        --d_ff $d_ff \
        --d_state $d_state \
        --learning_rate $learning_rate \
        --itr $itr > "$log_file" 2>&1

    if [ $? -eq 0 ]; then
        echo "âœ… æˆåŠŸå®Œæˆ: ${model_id}"
    else
        echo "âŒ è®­ç»ƒå¤±è´¥: ${model_id}ï¼ŒæŸ¥çœ‹æ—¥å¿—: $log_file"
    fi
    echo "---"
done

export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=2
enc_in=7
dec_in=7
c_out=7
data=ETTm1
root_path="./dataset/ETT-small/"
data_path="ETTm1.csv"
features=M
des='Exp'
itr=1

# ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡ï¼ˆä¿æŒé«˜ç²¾åº¦ä»»åŠ¡ä½¿ç”¨ç¨å¤§å­¦ä¹ çŽ‡ï¼‰
declare -A lr_map
lr_map[96]=0.00005
lr_map[192]=0.00005
lr_map[336]=0.00005
lr_map[720]=0.00005

# d_model å’Œ d_ff æ ¹æ® pred_len æ¡ä»¶è®¾ç½®ï¼ˆå¦‚ä½ åŽŸå§‹è„šæœ¬ä¸­ï¼š96æ­¥ç”¨256ï¼Œå…¶ä½™ç”¨128ï¼‰
# æˆ‘ä»¬é€šè¿‡å…³è”æ•°ç»„æ¥é…ç½®
declare -A d_model_map
d_model_map[96]=256
d_model_map[192]=128
d_model_map[336]=128
d_model_map[720]=128

declare -A d_ff_map
d_ff_map[96]=256
d_ff_map[192]=128
d_ff_map[336]=128
d_ff_map[720]=128

# d_state ç»Ÿä¸€ä¸º 2
d_state=2

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    d_model=${d_model_map[$pred_len]}
    d_ff=${d_ff_map[$pred_len]}
    model_id=ETTm1_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"
    
    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_state $d_state \
      --d_ff $d_ff \
      --itr $itr \
      --learning_rate $learning_rate > "$log_file" 2>&1

    echo "âœ… Training completed for ETTm1 | pred_len=$pred_len | LR=$learning_rate | d_model=$d_model | Log: $log_file"
done


export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=2
enc_in=7
dec_in=7
c_out=7
data=ETTm2
root_path="./dataset/ETT-small/"
data_path="ETTm2.csv"
features=M
des='Exp'
itr=1
d_state=2

# ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡ï¼ˆæ³¨æ„ï¼š336 æ˜¯ 0.00003ï¼Œå…¶ä½™ä¸º 0.00005ï¼‰
declare -A lr_map
lr_map[96]=0.00005
lr_map[192]=0.00005
lr_map[336]=0.00003
lr_map[720]=0.00005

# d_model é…ç½®ï¼š96 æ­¥ç”¨ 256ï¼Œå…¶ä½™ç”¨ 128
declare -A d_model_map
d_model_map[96]=256
d_model_map[192]=128
d_model_map[336]=128
d_model_map[720]=128

# d_ff é…ç½®ï¼šä¸Ž d_model ä¸€è‡´
declare -A d_ff_map
d_ff_map[96]=256
d_ff_map[192]=128
d_ff_map[336]=128
d_ff_map[720]=128

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    d_model=${d_model_map[$pred_len]}
    d_ff=${d_ff_map[$pred_len]}
    model_id=ETTm2_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"
    
    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_state $d_state \
      --d_ff $d_ff \
      --itr $itr \
      --learning_rate $learning_rate > "$log_file" 2>&1

    echo "âœ… Training completed for ETTm2 | pred_len=$pred_len | LR=$learning_rate | d_model=$d_model | Log: $log_file"
done

export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=3
enc_in=321
dec_in=321
c_out=321
data=custom
root_path="./dataset/electricity/"
data_path="electricity.csv"
features=M
des='Exp'
itr=1
d_model=512
d_ff=512
train_epochs=5
batch_size=16

# ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡
declare -A lr_map
lr_map[96]=0.001
lr_map[192]=0.0005
lr_map[336]=0.0005
lr_map[720]=0.0005

# d_state ä»…åœ¨ pred_len=96 æ—¶æ˜¾å¼è®¾ç½®ä¸º 16ï¼Œå…¶ä½™æƒ…å†µä¸ä¼ å‚ï¼ˆä¿æŒåŽŸå§‹è¡Œä¸ºï¼‰
# å› æ­¤æˆ‘ä»¬é€šè¿‡æ¡ä»¶åˆ¤æ–­æ¥å†³å®šæ˜¯å¦æ·»åŠ  --d_state å‚æ•°

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    model_id=ECL_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    # æž„å»ºåŸºç¡€å‘½ä»¤
    cmd="python -u run.py \
      --is_training 1 \
      --root_path \"$root_path\" \
      --data_path \"$data_path\" \
      --model_id \"$model_id\" \
      --model \"$model_name\" \
      --data \"$data\" \
      --features \"$features\" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des \"$des\" \
      --d_model $d_model \
      --d_ff $d_ff \
      --train_epochs $train_epochs \
      --batch_size $batch_size \
      --learning_rate $learning_rate \
      --itr $itr"

    # åªæœ‰ pred_len=96 æ—¶æ·»åŠ  --d_state 16
    if [ "$pred_len" -eq 96 ]; then
        cmd="$cmd --d_state 16"
    fi

    # æ‰§è¡Œå‘½ä»¤å¹¶é‡å®šå‘æ—¥å¿—
    eval $cmd > "$log_file" 2>&1

    echo "âœ… Training completed for ECL | pred_len=$pred_len | LR=$learning_rate $( [ "$pred_len" -eq 96 ] && echo "| d_state=16" ) | Log: $log_file"
done


export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=2
enc_in=8
dec_in=8
c_out=8
data=custom
root_path="./dataset/exchange_rate/"
data_path="exchange_rate.csv"
features=M
des='Exp'
itr=1
d_model=128
d_ff=128
batch_size=16  # æ‰€æœ‰ä»»åŠ¡éƒ½ä½¿ç”¨ batch_size=16

# ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡
declare -A lr_map
lr_map[96]=0.0001
lr_map[192]=0.0001
lr_map[336]=0.00005
lr_map[720]=0.00005

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    model_id=Exchange_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_ff $d_ff \
      --batch_size $batch_size \
      --learning_rate $learning_rate \
      --itr $itr > "$log_file" 2>&1

    echo "âœ… Training completed for Exchange | pred_len=$pred_len | LR=$learning_rate | Log: $log_file"
done


export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
pred_len_list=(96 192 336 720)
e_layers=2
enc_in=137
dec_in=137
c_out=137
data=Solar
root_path="./dataset/Solar/"
data_path="solar_AL.txt"
features=M
des='Exp'
itr=1
d_model=512
d_ff=512

# å›ºå®šå­¦ä¹ çŽ‡ï¼ˆåŽŸå§‹è„šæœ¬æœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤æˆ–æ¨¡åž‹å†…éƒ¨é»˜è®¤å€¼ï¼‰
# å¦‚æžœ run.py ä¸­æœ‰é»˜è®¤ learning_rateï¼Œåˆ™æ— éœ€è®¾ç½®ï¼›è‹¥éœ€æ˜¾å¼æŒ‡å®šï¼Œå¯æ·»åŠ ï¼š
# --learning_rate 0.0001  # ç¤ºä¾‹å€¼ï¼ŒåŽŸå§‹è„šæœ¬æœªæä¾›ï¼Œæ•…ä¸æ·»åŠ 

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    model_id=solar_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_ff $d_ff \
      --itr $itr > "$log_file" 2>&1

    echo "âœ… Training completed for Solar | pred_len=$pred_len | Log: $log_file"
done


export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=4
enc_in=862
dec_in=862
c_out=862
data=custom
root_path="./dataset/traffic/"
data_path="traffic.csv"
features=M
des='Exp'
itr=1
d_model=512
d_ff=512
batch_size=16  # æ‰€æœ‰ä»»åŠ¡å‡ä½¿ç”¨ batch_size=16

# ä¸åŒé¢„æµ‹é•¿åº¦å¯¹åº”çš„å­¦ä¹ çŽ‡
declare -A lr_map
lr_map[96]=0.001
lr_map[192]=0.001
lr_map[336]=0.002
lr_map[720]=0.0008

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    learning_rate=${lr_map[$pred_len]}
    model_id=traffic_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_ff $d_ff \
      --batch_size $batch_size \
      --learning_rate $learning_rate \
      --itr $itr > "$log_file" 2>&1

    echo "âœ… Training completed for Traffic | pred_len=$pred_len | LR=$learning_rate | Log: $log_file"
done

export CUDA_VISIBLE_DEVICES=0

# åˆ›å»ºæ—¥å¿—ç›®å½•
if [ ! -d "./logs" ]; then
    mkdir ./logs
fi

model_name=S_Mamba
seq_len=96
e_layers=3
enc_in=21
dec_in=21
c_out=21
data=custom
root_path="./dataset/weather/"
data_path="weather.csv"
features=M
des='Exp'
itr=1
d_model=512
d_ff=512
d_state=2
learning_rate=0.00005
train_epochs=5
batch_size=32  # å¦‚æžœ run.py æœ‰é»˜è®¤å€¼åˆ™å¯çœç•¥ï¼›è‹¥éœ€æ˜¾å¼è®¾ç½®ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä¼ å‚

# é¢„æµ‹é•¿åº¦åˆ—è¡¨
declare -a pred_len_list=(96 192 336 720)

# éåŽ†è®­ç»ƒ
for pred_len in "${pred_len_list[@]}"; do
    model_id=weather_${seq_len}_${pred_len}
    log_file="./logs/${model_name}_${model_id}.log"

    python -u run.py \
      --is_training 1 \
      --root_path "$root_path" \
      --data_path "$data_path" \
      --model_id "$model_id" \
      --model "$model_name" \
      --data "$data" \
      --features "$features" \
      --seq_len $seq_len \
      --pred_len $pred_len \
      --e_layers $e_layers \
      --enc_in $enc_in \
      --dec_in $dec_in \
      --c_out $c_out \
      --des "$des" \
      --d_model $d_model \
      --d_ff $d_ff \
      --d_state $d_state \
      --learning_rate $learning_rate \
      --train_epochs $train_epochs \
      --itr $itr > "$log_file" 2>&1

    echo "âœ… Training completed for Weather | pred_len=$pred_len | LR=$learning_rate | Log: $log_file"
done